"""
ç¢³æ’æ”¾è¨ˆç®—å·¥å…· v3.2 - Webç‰ˆæœ¬
å®Œæ•´åŠŸèƒ½ç‰ˆæœ¬

ä½œè€…: æ¥Šå‹¢è³¢
æ—¥æœŸ: 2024-12-27
"""

import streamlit as st
import pandas as pd
import numpy as np
from geopy.geocoders import Nominatim
from geopy.distance import geodesic
import requests
from docx import Document
from datetime import datetime
import matplotlib.pyplot as plt
import json
import time
from pathlib import Path
from io import BytesIO

# ========ã€é é¢è¨­å®šã€‘========
st.set_page_config(
    page_title="ç¢³æ’æ”¾è¨ˆç®—å·¥å…· v3.2",
    page_icon="ğŸŒ±",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ========ã€matplotlib ä¸­æ–‡å­—å‹è¨­å®šã€‘========
plt.rcParams["font.sans-serif"] = ["Microsoft JhengHei", "SimHei", "Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False

# ========ã€å¸¸æ•¸å®šç¾©ã€‘========
GWP_CH4 = 28
GWP_N2O = 265

FIXED_CO2_FACTORS = {
    "æ·é‹": 0.04, "å…¬è»Š": 0.04, "å¤§å®¢è»Š": 0.04, "é«˜éµ": 0.04, "ç«è»Š": 0.06,
    "æ‘©æ‰˜è»Š": 0.046, "é›»å‹•æ©Ÿè»Š": 0.025, "é›»å‹•è»Š": 0.078, "é£›æ©Ÿ": 2.1981, "èˆ¹": 2.606
}

RECOMMENDED_FUEL_TYPES = {
    "æ±½è»Š": "è»Šç”¨æ±½æ²¹", "èˆ¹": "æŸ´æ²¹", "é£›æ©Ÿ": "èˆªç©ºæ±½æ²¹",
    "æ‘©æ‰˜è»Š": "è»Šç”¨æ±½æ²¹", "å…¬è»Š": "æŸ´æ²¹", "å¤§å®¢è»Š": "æŸ´æ²¹"
}

# ========ã€æŒçºŒå­¸ç¿’ç³»çµ±ã€‘========
class ContinuousLearningSystem:
    """æŒçºŒå­¸ç¿’ç³»çµ± - è®“AIè¶Šç”¨è¶Šè°æ˜"""
    
    def __init__(self, storage_dir="./carbon_learning_data"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)
        
        # å„²å­˜æ–‡ä»¶è·¯å¾‘
        self.conversation_file = self.storage_dir / "conversations.json"
        self.company_profile_file = self.storage_dir / "company_profile.json"
        self.qa_database_file = self.storage_dir / "qa_database.json"
        self.usage_stats_file = self.storage_dir / "usage_stats.json"
        
        # è¼‰å…¥æ­·å²æ•¸æ“š
        self.conversations = self._load_json(self.conversation_file, [])
        self.company_profile = self._load_json(self.company_profile_file, {})
        self.qa_database = self._load_json(self.qa_database_file, [])
        self.usage_stats = self._load_json(self.usage_stats_file, {
            "total_runs": 0,
            "total_conversations": 0,
            "first_use": None,
            "last_use": None
        })
        
        # æ›´æ–°ä½¿ç”¨çµ±è¨ˆ
        self.usage_stats["total_runs"] += 1
        self.usage_stats["last_use"] = datetime.now().isoformat()
        if self.usage_stats["first_use"] is None:
            self.usage_stats["first_use"] = datetime.now().isoformat()
        
        self._save_json(self.usage_stats_file, self.usage_stats)
    
    def _load_json(self, filepath, default):
        """è¼‰å…¥JSONæ–‡ä»¶"""
        try:
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except:
            pass
        return default
    
    def _save_json(self, filepath, data):
        """å„²å­˜JSONæ–‡ä»¶"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"å„²å­˜å¤±æ•—: {e}")
    
    def analyze_data(self, detailed_results):
        """åˆ†æExcelæ•¸æ“šä¸¦æ›´æ–°å…¬å¸æª”æ¡ˆ"""
        if detailed_results.empty:
            return
        
        analysis = {
            "last_updated": datetime.now().isoformat(),
            "total_employees": len(detailed_results),
            "units": {},
            "transport_modes": {},
            "total_emissions": float(detailed_results['ç¸½æ’æ”¾é‡(kg CO2e)'].sum()),
            "avg_distance": float(detailed_results['è·é›¢(å…¬é‡Œ)'].mean()),
            "top_emitters": []
        }
        
        # æŒ‰å–®ä½çµ±è¨ˆ
        for unit, group in detailed_results.groupby('å–®ä½åç¨±'):
            analysis['units'][str(unit)] = {
                "employee_count": len(group),
                "total_emissions": float(group['ç¸½æ’æ”¾é‡(kg CO2e)'].sum()),
                "avg_distance": float(group['è·é›¢(å…¬é‡Œ)'].mean()),
                "dominant_transport": group['äº¤é€šæ–¹å¼'].mode()[0] if len(group) > 0 else "æœªçŸ¥"
            }
        
        # æŒ‰äº¤é€šæ–¹å¼çµ±è¨ˆ
        for transport, group in detailed_results.groupby('äº¤é€šæ–¹å¼'):
            analysis['transport_modes'][str(transport)] = {
                "user_count": len(group),
                "total_emissions": float(group['ç¸½æ’æ”¾é‡(kg CO2e)'].sum()),
                "avg_emissions": float(group['ç¸½æ’æ”¾é‡(kg CO2e)'].mean())
            }
        
        # æ‰¾å‡ºé«˜æ’æ”¾å“¡å·¥
        top_5 = detailed_results.nlargest(5, 'ç¸½æ’æ”¾é‡(kg CO2e)')
        for _, row in top_5.iterrows():
            analysis['top_emitters'].append({
                "employee": str(row['å“¡å·¥åç¨±']),
                "unit": str(row['å–®ä½åç¨±']),
                "transport": str(row['äº¤é€šæ–¹å¼']),
                "distance": float(row['è·é›¢(å…¬é‡Œ)']),
                "emissions": float(row['ç¸½æ’æ”¾é‡(kg CO2e)'])
            })
        
        self.company_profile = analysis
        self._save_json(self.company_profile_file, self.company_profile)
        
        return analysis
    
    def get_data_context(self):
        """å–å¾—æ•¸æ“šä¸Šä¸‹æ–‡"""
        if not self.company_profile:
            return "ç›®å‰æ²’æœ‰æ•¸æ“š"
        
        context = f"""
## å…¬å¸ç¢³æ’æ”¾æ•¸æ“šæ¦‚è¦½

ã€åŸºæœ¬è³‡è¨Šã€‘
- å“¡å·¥ç¸½æ•¸: {self.company_profile.get('total_employees', 0)} äºº
- ç¸½ç¢³æ’æ”¾: {self.company_profile.get('total_emissions', 0):.2f} kg CO2e
- å¹³å‡é€šå‹¤è·é›¢: {self.company_profile.get('avg_distance', 0):.2f} å…¬é‡Œ

ã€å„å–®ä½çµ±è¨ˆã€‘
"""
        for unit, data in self.company_profile.get('units', {}).items():
            context += f"""
{unit}:
  - äººæ•¸: {data['employee_count']} äºº
  - ç¢³æ’æ”¾: {data['total_emissions']:.2f} kg CO2e
  - å¹³å‡è·é›¢: {data['avg_distance']:.2f} km
  - ä¸»è¦äº¤é€šæ–¹å¼: {data['dominant_transport']}
"""
        
        return context
    
    def save_conversation(self, user_message, ai_response):
        """å„²å­˜å°è©±è¨˜éŒ„"""
        conversation = {
            "timestamp": datetime.now().isoformat(),
            "user": user_message,
            "ai": ai_response
        }
        
        self.conversations.append(conversation)
        self.usage_stats["total_conversations"] += 1
        
        if len(self.conversations) > 100:
            self.conversations = self.conversations[-100:]
        
        self._save_json(self.conversation_file, self.conversations)
        self._save_json(self.usage_stats_file, self.usage_stats)
    
    def get_conversation_history(self, last_n=5):
        """å–å¾—æœ€è¿‘çš„å°è©±æ­·å²"""
        if not self.conversations:
            return ""
        
        history = "\n## æœ€è¿‘å°è©±è¨˜éŒ„\n"
        for conv in self.conversations[-last_n:]:
            timestamp = conv['timestamp'][:19]
            history += f"\n[{timestamp}]\n"
            history += f"ç”¨æˆ¶: {conv['user']}\n"
            history += f"AI: {conv['ai'][:100]}...\n"
        
        return history
    
    def get_learning_summary(self):
        """å–å¾—å­¸ç¿’æ‘˜è¦"""
        return f"""
## ç³»çµ±å­¸ç¿’ç‹€æ…‹

ã€ä½¿ç”¨çµ±è¨ˆã€‘
- ç¸½åŸ·è¡Œæ¬¡æ•¸: {self.usage_stats['total_runs']}
- ç¸½å°è©±æ¬¡æ•¸: {self.usage_stats['total_conversations']}
- é¦–æ¬¡ä½¿ç”¨: {self.usage_stats.get('first_use', 'æœªçŸ¥')[:10]}
- æœ€å¾Œä½¿ç”¨: {self.usage_stats.get('last_use', 'æœªçŸ¥')[:10]}

ã€çŸ¥è­˜åº«ã€‘
- å°è©±æ­·å²: {len(self.conversations)} æ¢
- å•ç­”è³‡æ–™åº«: {len(self.qa_database)} çµ„

ã€å…¬å¸æª”æ¡ˆã€‘
- è³‡æ–™ç‹€æ…‹: {'å·²å»ºç«‹' if self.company_profile else 'æœªå»ºç«‹'}
- æœ€å¾Œæ›´æ–°: {self.company_profile.get('last_updated', 'å¾æœª')[:19] if self.company_profile else 'å¾æœª'}
"""

# ========ã€å·¥å…·å‡½æ•¸ã€‘========

def is_na_value(value):
    """æª¢æŸ¥æ˜¯å¦ç‚ºNAå€¼"""
    if pd.isna(value):
        return True
    if value is None:
        return True
    value_str = str(value).strip().upper()
    if value_str == '' or value_str == 'NA' or value_str == 'NAN':
        return True
    return False

@st.cache_resource
def get_geolocator():
    """å–å¾—åœ°ç†ç·¨ç¢¼å™¨ï¼ˆå¿«å–ï¼‰"""
    return Nominatim(user_agent="carbon_emission_web_app")

def get_coordinates(address):
    """å–å¾—åœ°é»åº§æ¨™"""
    if is_na_value(address):
        return None
    try:
        geolocator = get_geolocator()
        location = geolocator.geocode(str(address), timeout=10)
        return (location.latitude, location.longitude) if location else None
    except:
        return None

def auto_fix_excel_data(df):
    """è‡ªå‹•ä¿®å¾©Excelè³‡æ–™"""
    df_fixed = df.copy()
    fix_log = []
    error_log = []
    
    transport_keywords = ['æ±½è»Š', 'ç«è»Š', 'æ·é‹', 'é«˜éµ', 'å…¬è»Š', 'å¤§å®¢è»Š', 
                          'æ‘©æ‰˜è»Š', 'æ©Ÿè»Š', 'é£›æ©Ÿ', 'èˆ¹', 'é›»å‹•']
    
    # æª¢æŸ¥æ¬„ä½éŒ¯ç½®
    swap_count = 0
    for idx, row in df_fixed.iterrows():
        unit_value = str(row['å–®ä½åç¨±'])
        transport_value = str(row['äº¤é€šæ–¹å¼'])
        
        unit_has_transport = any(keyword in unit_value for keyword in transport_keywords)
        transport_is_code = len(transport_value) <= 3
        
        if unit_has_transport and transport_is_code:
            df_fixed.at[idx, 'å–®ä½åç¨±'] = transport_value
            df_fixed.at[idx, 'äº¤é€šæ–¹å¼'] = unit_value
            fix_log.append(f"âœ“ å“¡å·¥ {row['å“¡å·¥åç¨±']}: å·²äº¤æ›æ¬„ä½")
            swap_count += 1
    
    # æª¢æŸ¥å‡ºç™¼é»NA
    for idx, row in df_fixed.iterrows():
        if is_na_value(row['å‡ºç™¼é»']):
            error_log.append(f"å“¡å·¥ {row['å“¡å·¥åç¨±']}: âŒ å‡ºç™¼é»ç‚ºNA")
    
    # å¡«è£œç›®çš„åœ°NA
    dest_na_indices = []
    for idx, row in df_fixed.iterrows():
        if is_na_value(row['ç›®çš„åœ°']):
            dest_na_indices.append((idx, row['å“¡å·¥åç¨±']))
    
    if dest_na_indices:
        non_na_dests = [row['ç›®çš„åœ°'] for _, row in df_fixed.iterrows() if not is_na_value(row['ç›®çš„åœ°'])]
        if non_na_dests:
            unique_dests = list(set(non_na_dests))
            if len(unique_dests) == 1:
                fill_value = unique_dests[0]
                for idx, name in dest_na_indices:
                    df_fixed.at[idx, 'ç›®çš„åœ°'] = fill_value
                fix_log.append(f"âœ“ ç›®çš„åœ°: å·²å¡«è£œ {len(dest_na_indices)} å€‹NA")
    
    return df_fixed, fix_log, error_log

def calculate_emissions(distance, fuel_data, fuel_type, transport_mode):
    """è¨ˆç®—ç¢³æ’æ”¾"""
    if transport_mode in FIXED_CO2_FACTORS:
        co2 = distance * FIXED_CO2_FACTORS[transport_mode]
        details = f"è·é›¢({distance:.2f}km) Ã— COâ‚‚ä¿‚æ•¸({FIXED_CO2_FACTORS[transport_mode]}) = {co2:.2f} kg COâ‚‚"
        return {"CO2": co2, "CH4": 0, "N2O": 0, "Total": co2, "Details": details}
    
    matched_fuel = fuel_data[fuel_data["ç‡ƒæ–™åˆ¥"] == fuel_type]
    if not matched_fuel.empty:
        co2_factor = matched_fuel.iloc[0]["CO2"]
        ch4_factor = matched_fuel.iloc[0]["CH4"]
        n2o_factor = matched_fuel.iloc[0]["N2O"]
        
        co2 = distance * co2_factor
        ch4 = distance * ch4_factor * GWP_CH4
        n2o = distance * n2o_factor * GWP_N2O
        total = co2 + ch4 + n2o
        
        details = (
            f"CO2: {distance:.2f}km Ã— {co2_factor} = {co2:.2f} kg\n"
            f"CH4: {distance:.2f}km Ã— {ch4_factor} Ã— {GWP_CH4} = {ch4:.2f} kg\n"
            f"N2O: {distance:.2f}km Ã— {n2o_factor} Ã— {GWP_N2O} = {n2o:.2f} kg\n"
            f"ç¸½è¨ˆ = {total:.2f} kg COâ‚‚e"
        )
        return {"CO2": co2, "CH4": ch4, "N2O": n2o, "Total": total, "Details": details}
    
    return {"CO2": 0, "CH4": 0, "N2O": 0, "Total": 0, "Details": "ç„¡è³‡æ–™"}

def chat_with_ai(message, api_key, learning_system):
    """AIå°è©±"""
    if not api_key:
        return "è«‹å…ˆåœ¨å´é‚Šæ¬„è¨­å®š OpenAI API é‡‘é‘°"
    
    try:
        # ç¢³æ’æ”¾å°ˆå®¶æç¤ºè©
        base_prompt = """ä½ æ˜¯ç¢³æ’æ”¾è¨ˆç®—å’Œæ¸›ç¢³ç­–ç•¥çš„å°ˆæ¥­é¡§å•ã€‚

ç²¾é€š ISO 14064-1:2018 å’Œ IPCC æŒ‡å—ã€‚

è«‹æä¾›å…·é«”ã€å¯è¡Œã€é‡åŒ–çš„å»ºè­°ã€‚"""
        
        # æ·»åŠ æ•¸æ“šä¸Šä¸‹æ–‡
        data_context = learning_system.get_data_context()
        if data_context != "ç›®å‰æ²’æœ‰æ•¸æ“š":
            base_prompt += f"\n\n{data_context}"
        
        # æ·»åŠ å°è©±æ­·å²
        conversation_history = learning_system.get_conversation_history(last_n=3)
        if conversation_history:
            base_prompt += f"\n\n{conversation_history}"
        
        # å‘¼å« OpenAI API
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        data = {
            "model": "gpt-3.5-turbo",
            "messages": [
                {"role": "system", "content": base_prompt},
                {"role": "user", "content": message}
            ],
            "max_tokens": 800,
            "temperature": 0.3
        }
        
        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            ai_response = result['choices'][0]['message']['content']
            
            # å„²å­˜å°è©±
            learning_system.save_conversation(message, ai_response)
            
            return ai_response
        else:
            return f"API éŒ¯èª¤ï¼š{response.status_code}"
            
    except Exception as e:
        return f"éŒ¯èª¤ï¼š{str(e)}"

def generate_word_report(detailed_results, report_type='simple'):
    """ç”ŸæˆWordå ±è¡¨"""
    doc = Document()
    doc.add_heading(f"ç¢³æ’æ”¾æˆç¸¾å–®ï¼ˆ{report_type}ç‰ˆï¼‰", level=1)
    
    # æ‘˜è¦
    total_emissions = detailed_results['ç¸½æ’æ”¾é‡(kg CO2e)'].sum()
    avg_distance = detailed_results['è·é›¢(å…¬é‡Œ)'].mean()
    
    doc.add_paragraph(f"ç¸½æ’æ”¾é‡: {total_emissions:.2f} kg CO2e")
    doc.add_paragraph(f"å¹³å‡è·é›¢: {avg_distance:.2f} å…¬é‡Œ")
    doc.add_paragraph(f"ç¸½äººæ•¸: {len(detailed_results)} äºº")
    doc.add_paragraph("")
    
    # è¡¨æ ¼
    table = doc.add_table(rows=1, cols=6)
    table.style = 'Light Grid Accent 1'
    
    headers = ["å“¡å·¥åç¨±", "å–®ä½åç¨±", "äº¤é€šæ–¹å¼", "ç‡ƒæ–™ç¨®é¡", "è·é›¢(å…¬é‡Œ)", "ç¸½æ’æ”¾é‡(kg CO2e)"]
    for i, header in enumerate(headers):
        table.rows[0].cells[i].text = header
    
    for _, row in detailed_results.iterrows():
        cells = table.add_row().cells
        cells[0].text = str(row["å“¡å·¥åç¨±"])
        cells[1].text = str(row["å–®ä½åç¨±"])
        cells[2].text = str(row["äº¤é€šæ–¹å¼"])
        cells[3].text = str(row["ç‡ƒæ–™ç¨®é¡"])
        cells[4].text = f"{row['è·é›¢(å…¬é‡Œ)']:.2f}"
        cells[5].text = f"{row['ç¸½æ’æ”¾é‡(kg CO2e)']:.2f}"
    
    # å„²å­˜åˆ°BytesIO
    bio = BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio

# ========ã€åˆå§‹åŒ–ã€‘========

# åˆå§‹åŒ– session state
if 'learning_system' not in st.session_state:
    st.session_state.learning_system = ContinuousLearningSystem()

if 'detailed_results' not in st.session_state:
    st.session_state.detailed_results = pd.DataFrame()

if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# ========ã€ä¸»ä»‹é¢ã€‘========

# æ¨™é¡Œ
st.markdown('<h1 style="text-align: center; color: #2E7D32;">ğŸŒ± ç¢³æ’æ”¾è¨ˆç®—å·¥å…· v3.2</h1>', unsafe_allow_html=True)
st.markdown('<p style="text-align: center; color: #666;">æ™ºèƒ½å­¸ç¿’ç‰ˆ - Web Edition</p>', unsafe_allow_html=True)

# å´é‚Šæ¬„
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    # API é‡‘é‘°
    st.subheader("AI è¨­å®š")
    api_key = st.text_input(
        "OpenAI API é‡‘é‘°ï¼ˆé¸å¡«ï¼‰",
        type="password",
        help="ç”¨æ–¼ AI å°è©±åŠŸèƒ½"
    )
    
    if api_key:
        st.success("âœ… API é‡‘é‘°å·²è¨­å®š")
    else:
        st.info("ğŸ’¡ ä¸è¨­å®šä¹Ÿå¯ä»¥é€²è¡Œè¨ˆç®—")
    
    # å­¸ç¿’ç‹€æ…‹
    st.markdown("---")
    st.subheader("ğŸ“Š ä½¿ç”¨çµ±è¨ˆ")
    st.metric("è¨ˆç®—æ¬¡æ•¸", st.session_state.learning_system.usage_stats['total_runs'])
    st.metric("å°è©±æ¬¡æ•¸", st.session_state.learning_system.usage_stats['total_conversations'])
    
    # å­¸ç¿’ç‹€æ…‹è©³æƒ…
    if st.button("æŸ¥çœ‹è©³ç´°å­¸ç¿’ç‹€æ…‹"):
        st.text(st.session_state.learning_system.get_learning_summary())

# ä¸»è¦åˆ†é 
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š è¨ˆç®—", "ğŸ’¬ AI å°è©±", "ğŸ“ˆ è¦–è¦ºåŒ–", "ğŸ“„ å ±è¡¨"])

# ========ã€Tab 1: è¨ˆç®—ã€‘========
with tab1:
    st.header("ğŸ“Š ç¢³æ’æ”¾è¨ˆç®—")
    
    # æª”æ¡ˆä¸Šå‚³
    uploaded_file = st.file_uploader(
        "ä¸Šå‚³ Excel æª”æ¡ˆ",
        type=['xlsx', 'xls'],
        help="è«‹ç¢ºä¿æª”æ¡ˆåŒ…å«å¿…è¦æ¬„ä½"
    )
    
    if uploaded_file:
        st.success(f"âœ… æª”æ¡ˆå·²ä¸Šå‚³: {uploaded_file.name}")
        
        # é¡¯ç¤ºåŸå§‹è³‡æ–™é è¦½
        with st.expander("ğŸ“‹ åŸå§‹è³‡æ–™é è¦½"):
            try:
                preview_df = pd.read_excel(uploaded_file, sheet_name='å·¥ä½œè¡¨2')
                st.dataframe(preview_df.head())
            except Exception as e:
                st.error(f"ç„¡æ³•é è¦½: {e}")
        
        # è¨ˆç®—æŒ‰éˆ•
        if st.button("ğŸš€ é–‹å§‹è¨ˆç®—", type="primary"):
            with st.spinner("è¨ˆç®—ä¸­..."):
                try:
                    # è®€å–è³‡æ–™
                    transport_data = pd.read_excel(uploaded_file, sheet_name='å·¥ä½œè¡¨2')
                    fuel_data = pd.read_excel(uploaded_file, sheet_name='å·¥ä½œè¡¨6')
                    
                    # ä¿®å¾©è³‡æ–™
                    transport_data, fix_log, error_log = auto_fix_excel_data(transport_data)
                    
                    # è¨ˆç®—æ’æ”¾é‡
                    detailed_results_list = []
                    progress_bar = st.progress(0)
                    
                    for idx, row in transport_data.iterrows():
                        progress_bar.progress((idx + 1) / len(transport_data))
                        
                        if is_na_value(row['å‡ºç™¼é»']) or is_na_value(row['ç›®çš„åœ°']):
                            detailed_results_list.append({
                                'å“¡å·¥åç¨±': row['å“¡å·¥åç¨±'],
                                'å–®ä½åç¨±': row['å–®ä½åç¨±'],
                                'äº¤é€šæ–¹å¼': row['äº¤é€šæ–¹å¼'],
                                'ç‡ƒæ–™ç¨®é¡': '-',
                                'è·é›¢(å…¬é‡Œ)': 0,
                                'ç¸½æ’æ”¾é‡(kg CO2e)': 0,
                                'è¨ˆç®—éç¨‹': 'âŒ ç„¡æ³•è¨ˆç®—',
                                'éŒ¯èª¤è¨Šæ¯': 'è«‹ç¢ºèªå‡ºç™¼é»/ç›®çš„åœ°'
                            })
                            continue
                        
                        # å–å¾—åº§æ¨™ä¸¦è¨ˆç®—è·é›¢
                        origin_coords = get_coordinates(row['å‡ºç™¼é»'])
                        destination_coords = get_coordinates(row['ç›®çš„åœ°'])
                        
                        if origin_coords and destination_coords:
                            distance = geodesic(origin_coords, destination_coords).km
                            fuel_type = RECOMMENDED_FUEL_TYPES.get(row['äº¤é€šæ–¹å¼'], "è»Šç”¨æ±½æ²¹")
                            emissions = calculate_emissions(distance, fuel_data, fuel_type, row['äº¤é€šæ–¹å¼'])
                            
                            detailed_results_list.append({
                                'å“¡å·¥åç¨±': row['å“¡å·¥åç¨±'],
                                'å–®ä½åç¨±': row['å–®ä½åç¨±'],
                                'äº¤é€šæ–¹å¼': row['äº¤é€šæ–¹å¼'],
                                'ç‡ƒæ–™ç¨®é¡': fuel_type,
                                'è·é›¢(å…¬é‡Œ)': round(distance, 2),
                                'ç¸½æ’æ”¾é‡(kg CO2e)': round(emissions['Total'], 2),
                                'è¨ˆç®—éç¨‹': emissions['Details'],
                                'éŒ¯èª¤è¨Šæ¯': ''
                            })
                    
                    progress_bar.empty()
                    
                    # å„²å­˜çµæœ
                    st.session_state.detailed_results = pd.DataFrame(detailed_results_list)
                    
                    # åˆ†ææ•¸æ“š
                    st.session_state.learning_system.analyze_data(st.session_state.detailed_results)
                    
                    st.success("âœ… è¨ˆç®—å®Œæˆï¼")
                    st.balloons()
                    
                    # é¡¯ç¤ºä¿®å¾©è¨˜éŒ„
                    if fix_log:
                        with st.expander("ğŸ”§ è³‡æ–™ä¿®å¾©è¨˜éŒ„"):
                            for log in fix_log:
                                st.text(log)
                    
                    if error_log:
                        with st.expander("âš ï¸ éœ€è¦ç¢ºèªçš„é …ç›®"):
                            for log in error_log:
                                st.warning(log)
                    
                except Exception as e:
                    st.error(f"âŒ è¨ˆç®—å¤±æ•—: {e}")
        
        # é¡¯ç¤ºçµæœ
        if not st.session_state.detailed_results.empty:
            st.markdown("---")
            st.subheader("ğŸ“Š è¨ˆç®—çµæœ")
            
            # çµ±è¨ˆæ‘˜è¦
            col1, col2, col3 = st.columns(3)
            
            results = st.session_state.detailed_results
            
            with col1:
                total_emissions = results['ç¸½æ’æ”¾é‡(kg CO2e)'].sum()
                st.metric("ç¸½æ’æ”¾é‡", f"{total_emissions:.2f} kg CO2e")
            
            with col2:
                avg_distance = results['è·é›¢(å…¬é‡Œ)'].mean()
                st.metric("å¹³å‡è·é›¢", f"{avg_distance:.2f} km")
            
            with col3:
                st.metric("ç¸½äººæ•¸", f"{len(results)} äºº")
            
            # è©³ç´°çµæœè¡¨æ ¼
            st.markdown("### è©³ç´°çµæœ")
            st.dataframe(
                results[['å“¡å·¥åç¨±', 'å–®ä½åç¨±', 'äº¤é€šæ–¹å¼', 'è·é›¢(å…¬é‡Œ)', 'ç¸½æ’æ”¾é‡(kg CO2e)']],
                use_container_width=True
            )
            
            # ä¸‹è¼‰CSV
            csv = results.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                "ğŸ“¥ ä¸‹è¼‰çµæœ (CSV)",
                csv,
                "ç¢³æ’æ”¾è¨ˆç®—çµæœ.csv",
                "text/csv"
            )

# ========ã€Tab 2: AI å°è©±ã€‘========
with tab2:
    st.header("ğŸ’¬ èˆ‡ AI äº’å‹•")
    
    if not api_key:
        st.warning("âš ï¸ è«‹å…ˆåœ¨å´é‚Šæ¬„è¨­å®š OpenAI API é‡‘é‘°")
    else:
        # é¡¯ç¤ºæ•¸æ“šæ¦‚è¦½
        data_summary = st.session_state.learning_system.get_data_context()
        if data_summary != "ç›®å‰æ²’æœ‰æ•¸æ“š":
            with st.expander("ğŸ“Š AIå·²è¼‰å…¥çš„æ•¸æ“šæ‘˜è¦"):
                st.text(data_summary[:500] + "...")
        
        # å°è©±è¨˜éŒ„
        for chat in st.session_state.chat_history:
            with st.chat_message(chat['role']):
                st.write(chat['content'])
        
        # è¼¸å…¥æ¡†
        user_input = st.chat_input("è¼¸å…¥æ‚¨çš„å•é¡Œ...")
        
        if user_input:
            # é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯
            with st.chat_message("user"):
                st.write(user_input)
            
            st.session_state.chat_history.append({
                'role': 'user',
                'content': user_input
            })
            
            # ç²å– AI å›æ‡‰
            with st.chat_message("assistant"):
                with st.spinner("æ€è€ƒä¸­..."):
                    response = chat_with_ai(user_input, api_key, st.session_state.learning_system)
                    st.write(response)
            
            st.session_state.chat_history.append({
                'role': 'assistant',
                'content': response
            })
            
            st.rerun()

# ========ã€Tab 3: è¦–è¦ºåŒ–ã€‘========
with tab3:
    st.header("ğŸ“ˆ æ•¸æ“šè¦–è¦ºåŒ–")
    
    if not st.session_state.detailed_results.empty:
        results = st.session_state.detailed_results
        
        # åœ“é¤…åœ–ï¼šå„å–®ä½æ’æ”¾å æ¯”
        st.subheader("ğŸ¥§ å„å–®ä½ç¢³æ’æ”¾å æ¯”")
        
        unit_emissions = results.groupby('å–®ä½åç¨±')['ç¸½æ’æ”¾é‡(kg CO2e)'].sum()
        
        fig, ax = plt.subplots(figsize=(10, 6))
        colors = plt.cm.Set3(range(len(unit_emissions)))
        
        wedges, texts, autotexts = ax.pie(
            unit_emissions.values,
            labels=unit_emissions.index,
            autopct='%1.1f%%',
            colors=colors,
            startangle=90
        )
        
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_weight('bold')
        
        ax.set_title('å„å–®ä½ç¢³æ’æ”¾å æ¯”', fontsize=14, weight='bold')
        plt.tight_layout()
        st.pyplot(fig)
        
        # é•·æ¢åœ–ï¼šå„äº¤é€šæ–¹å¼æ’æ”¾é‡
        st.subheader("ğŸ“Š å„äº¤é€šæ–¹å¼ç¢³æ’æ”¾é‡")
        
        transport_emissions = results.groupby('äº¤é€šæ–¹å¼')['ç¸½æ’æ”¾é‡(kg CO2e)'].sum().sort_values(ascending=False)
        
        fig2, ax2 = plt.subplots(figsize=(10, 6))
        bars = ax2.bar(transport_emissions.index, transport_emissions.values, color='#2E7D32')
        
        ax2.set_xlabel('äº¤é€šæ–¹å¼', fontsize=12)
        ax2.set_ylabel('ç¸½æ’æ”¾é‡ (kg CO2e)', fontsize=12)
        ax2.set_title('å„äº¤é€šæ–¹å¼ç¢³æ’æ”¾é‡', fontsize=14, weight='bold')
        
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}',
                    ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        st.pyplot(fig2)
        
    else:
        st.info("ğŸ“Š è«‹å…ˆåœ¨ã€Œè¨ˆç®—ã€é é¢å®Œæˆè¨ˆç®—")

# ========ã€Tab 4: å ±è¡¨ã€‘========
with tab4:
    st.header("ğŸ“„ å ±è¡¨ç”Ÿæˆ")
    
    if not st.session_state.detailed_results.empty:
        # å ±è¡¨é¡å‹é¸æ“‡
        report_type = st.radio(
            "é¸æ“‡å ±è¡¨é¡å‹",
            ["ç°¡æ˜“ç‰ˆ", "è©³ç´°ç‰ˆ"],
            horizontal=True
        )
        
        if st.button("ç”Ÿæˆ Word å ±è¡¨"):
            with st.spinner("ç”Ÿæˆä¸­..."):
                try:
                    doc_bio = generate_word_report(
                        st.session_state.detailed_results,
                        report_type
                    )
                    
                    st.download_button(
                        "ğŸ“¥ ä¸‹è¼‰ Word å ±è¡¨",
                        doc_bio,
                        f"ç¢³æ’æ”¾å ±è¡¨_{report_type}_{datetime.now().strftime('%Y%m%d')}.docx",
                        "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )
                    
                    st.success("âœ… å ±è¡¨å·²ç”Ÿæˆï¼")
                    
                except Exception as e:
                    st.error(f"âŒ ç”Ÿæˆå¤±æ•—: {e}")
    else:
        st.info("ğŸ“Š è«‹å…ˆåœ¨ã€Œè¨ˆç®—ã€é é¢å®Œæˆè¨ˆç®—")

# é è…³
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>ç¢³æ’æ”¾è¨ˆç®—å·¥å…· v3.2 - Webç‰ˆæœ¬</p>
    <p>Â© 2024 æ¥Šå‹¢è³¢ | ä¸­åŸå¤§å­¸å·¥æ¥­èˆ‡ç³»çµ±å·¥ç¨‹å­¸ç³»</p>
</div>
""", unsafe_allow_html=True)
